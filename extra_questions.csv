chapter,section,subsection,question,paragraph,answer
Regular Expressions,"",,For what purpose does ELIZA use pattern matching?,"The dialogue above is from ELIZA, an early natural language processing system ELIZA that could carry on a limited conversation with a user by imitating the responses of a Rogerian psychotherapist (Weizenbaum, 1966). ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like \""I need X\"" and translate them into suitable outputs like \""What would it mean to you if you got X?\"". This simple technique succeeds in this domain because ELIZA doesn't actually need to know anything to mimic a Rogerian psychotherapist. As Weizenbaum notes, this is one of the few dialogue genres where listeners can act as if they know nothing of the world. Eliza's mimicry of human conversation was remarkably successful: many people who interacted with ELIZA came to believe that it really understood them and their problems, many continued to believe in ELIZA's abilities even after the program's operation was explained to them (Weizenbaum, 1976), and even today such chatbots are a fun diversion.","ELIZA is a surprisingly simple program that uses pattern matching to recognize phrases like \""I need X\"" and translate them into suitable outputs like \""What would it mean to you if you got X?\""."
Nonesense,,,From which country comes the sport football? ,*** This is a nonesense question ***,*** This is a nonesense question ***
Regular Expressions,Regular Expressions,"Disjunction, Grouping, and Precedence",What is another name for the disjunction operator?,"Suppose we need to search for texts about pets; perhaps we are particularly interested in cats and dogs. In such a case, we might want to search for either the string cat or the string dog. Since we can't use the square brackets to search for ""cat or dog"" (why can't we say /[catdog]/?), we need a new operator, the disjunction operator, also disjunction called the pipe symbol |. The pattern /cat|dog/ matches either the string cat or the string dog."," the disjunction operator, also disjunction called the pipe symbol |."
Regular Expressions,Corpora,,What do we mean with the term 'code switching'? ,"It's also quite common for speakers or writers to use multiple languages in a single communicative act, a phenomenon called code switching. Code switching (2.2) Por primera vez veo a @username actually being hateful! it was beautiful:)","It's also quite common for speakers or writers to use multiple languages in a single communicative act, a phenomenon called code switching."
N-gram Language Models,Evaluating Language Models,,What is an extrinsic evaluation?,"The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation. Extrinsic evaluation is the only way to extrinsic evaluation know if a particular improvement in a component is really going to help the task at hand. Thus, for speech recognition, we can compare the performance of two language models by running the speech recognizer twice, once with each language model, and seeing which gives the more accurate transcription.",The best way to evaluate the performance of a language model is to embed it in an application and measure how much the application improves. Such end-to-end evaluation is called extrinsic evaluation.
N-gram Language Models,Evaluating Language Models,,What is an intrinsic evaluation?,"Unfortunately, running big NLP systems end-to-end is often very expensive. Instead, it would be nice to have a metric that can be used to quickly evaluate potential improvements in a language model. An intrinsic evaluation metric is one that mea-intrinsic evaluation sures the quality of a model independent of any application.",An intrinsic evaluation metric is one that mea-intrinsic evaluation sures the quality of a model independent of any application.
N-gram Language Models,Sampling sentences from a language model,,"What does ""sampling"" mean?","One important way to visualize what kind of knowledge a language model embodies is to sample from it. Sampling from a distribution means to choose random points sampling according to their likelihood. Thus sampling from a language model-which represents a distribution over sentences-means to generate some sentences, choosing each sentence according to its likelihood as defined by the model. Thus we are more likely to generate sentences that the model thinks have a high probability and less likely to generate sentences that the model thinks have a low probability.",Sampling from a distribution means to choose random points sampling according to their likelihood.
N-gram Language Models,Huge Language Models and Stupid Backoff,,How can one build approximate language models?,"An n-gram language model can also be shrunk by pruning, for example only storing n-grams with counts greater than some threshold (such as the count threshold of 40 used for the Google n-gram release) or using entropy to prune less-important n-grams (Stolcke, 1998) . Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007) .","Another option is to build approximate language models using techniques like Bloom filters (Talbot and Osborne 2007, Church et al. 2007) ."
N-gram Language Models,Kneser-Ney Smoothing,"",What is absolute discounting?,"Kneser-Ney has its roots in a method called absolute discounting. Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams.
",Recall that discounting of the counts for frequent n-grams is necessary to save some probability mass for the smoothing algorithm to distribute to the unseen n-grams.
Nonesense,,,How old is the king of the Netherlands,*** This is a nonesense question ***,*** This is a nonesense question ***
Naive Bayes and Sentiment Classification,Training the Naive Bayes Classifier,"",What can we do to solve the problem where the probability of the class will be zero when one likelihood has a zero probability?,"But since naive Bayes naively multiplies all the feature likelihoods together, zero probabilities in the likelihood term for any class will cause the probability of the class to be zero, no matter the other evidence! The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3. While Laplace smoothing is usually replaced by more sophisticated smoothing algorithms in language modeling, it is commonly used in naive Bayes text categorization:P",The simplest solution is the add-one (Laplace) smoothing introduced in Chapter 3.
Naive Bayes and Sentiment Classification,Statistical Significance Testing,The Paired Bootstrap Test,"What do we mean by ""bootstrapping""?","The bootstrap test (Efron and Tibshirani, 1993) can apply to any metric; from prebootstrap test cision, recall, or F1 to the BLEU metric used in machine translation. The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with bootstrapping replacement (called bootstrap samples) from an original larger sample. The intuition of the bootstrap test is that we can create many virtual test sets from an observed test set by repeatedly sampling from it. The method only makes the assumption that the sample is representative of the population.",The word bootstrapping refers to repeatedly drawing large numbers of smaller samples with bootstrapping replacement (called bootstrap samples) from an original larger sample.
Nonesense,"",,From which country do Ferraries come from?,*** This is a nonesense question ***,*** This is a nonesense question ***
Logistic Regression,,,What is the difference between naive Bayes and logistic regression?,Generative and Discriminative Classifiers: The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier.,The most important difference between naive Bayes and logistic regression is that logistic regression is a discriminative classifier while naive Bayes is a generative classifier.
Logistic Regression,,,What is a discriminative model?,"A discriminative model, by contrast, is only trying to learn to distinguish the classes (perhaps without learning much about them). So maybe all the dogs in the training data are wearing collars and the cats aren’t. If that one feature neatly separates the classes, the model is satisfied. If you ask such a model what it knows about cats all it can say is that they don’t wear collars.","A discriminative model, by contrast, is only trying to learn to distinguish the classes"
Logistic Regression,Classification: the Sigmoid,Example: Sentiment Classification,What is the task of period disambiguation?,"Logistic regression is commonly applied to all sorts of NLP tasks, and any property of the input can be a feature. Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes EOS (end-of-sentence) and not-EOS. We might use features like x 1 below expressing that the current word is lower case (perhaps with a positive weight), or that the current word is in our abbreviations dictionary (""Prof."") (perhaps with a negative weight). A feature can also express a quite complex combination of properties. For example a period following an upper case word is likely to be an EOS, but if the word itself is St. and the previous word is capitalized, then the period is likely part of a shortening of the word street. ","Consider the task of period disambiguation: deciding if a period is the end of a sentence or part of a word, by classifying each period into one of two classes EOS (end-of-sentence) and not-EOS."
Logistic Regression,Gradient Descent,,What is special about a function that is convex?,"For logistic regression, this loss function is conveniently convex. A convex funcconvex tion has just one minimum; there are no local minima to get stuck in, so gradient descent starting from any point is guaranteed to find the minimum. (By contrast, the loss for multi-layer neural networks is non-convex, and gradient descent may get stuck in local minima for neural network training and never find the global optimum.) Although the algorithm (and the concept of gradient) are designed for direction vectors, let's first consider a visualization of the case where the parameter of our system is just a single scalar w, shown in Fig. 5 .3.",A convex funcconvex tion has just one minimum;
Nonesense,,,What is the capital of the Netherlands?,,*** This is a nonesense question ***
Vector Semantics and Embeddings,,,What does the distributional hypothesis stand for?,"The asphalt that Los Angeles is famous for occurs mainly on its freeways. But in the middle of the city is another patch of asphalt, the La Brea tar pits, and this asphalt preserves millions of fossil bones from the last of the Ice Ages of the Pleistocene Epoch. One of these fossils is the Smilodon, or saber-toothed tiger, instantly recognizable by its long canines. Five million years ago or so, a completely different sabre-tooth tiger called Thylacosmilus lived in Argentina and other parts of South America. Thylacosmilus was a marsupial whereas Smilodon was a placental mammal, but Thylacosmilus had the same long upper canines and, like Smilodon, had a protective bone flange on the lower jaw. The similarity of these two mammals is one of many examples of parallel or convergent evolution, in which particular contexts or environments lead to the evolution of very similar structures in different species (Gould, 1980) . The role of context is also important in the similarity of a less biological kind of organism: the word. Words that occur in similar contexts tend to have similar meanings. This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis. The hypothesis was distributional hypothesis first formulated in the 1950s by linguists like Joos (1950), Harris (1954) , and Firth (1957), who noticed that words which are synonyms (like oculist and eye-doctor) tended to occur in the same environment (e.g., near words like eye or examined) with the amount of meaning difference between two words ""corresponding roughly to the amount of difference in their environments"" (Harris, 1954, 157) . In this chapter we introduce vector semantics, which instantiates this linguistic vector semantics hypothesis by learning representations of the meaning of words, called embeddings, embeddings directly from their distributions in texts. These representations are used in every natural language processing application that makes use of meaning, and the static embeddings we introduce here underlie the more powerful dynamic or contextualized embeddings like BERT that we will see in Chapter 11.",This link between similarity in how words are distributed and similarity in what they mean is called the distributional hypothesis.
Vector Semantics and Embeddings,Lexical Semantics,,When are two words synonyms?,"Synonymy One important component of word meaning is the relationship between word senses. For example when one word has a sense whose meaning is identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms. Synonyms include such pairs as couch/sofa vomit/throw up filbert/hazelnut car/automobile A more formal definition of synonymy (between words rather than senses) is that two words are synonymous if they are substitutable for one another in any sentence without changing the truth conditions of the sentence, the situations in which the sentence would be true. We often say in this case that the two words have the same propositional meaning.","For example when one word has a sense whose meaning is identical to a sense of another word, or nearly identical, we say the two senses of those two words are synonyms."
Vector Semantics and Embeddings,Lexical Semantics,,Why is similarity useful for semantic tasks?,"Word Similarity While words don't have many synonyms, most words do have lots of similar words. Cat is not a synonym of dog, but cats and dogs are certainly similar words. In moving from synonymy to similarity, it will be useful to shift from talking about relations between word senses (like synonymy) to relations between words (like similarity). Dealing with words avoids having to commit to a particular representation of word senses, which will turn out to simplify our task. The notion of word similarity is very useful in larger semantic tasks. Knowing similarity how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization. One way of getting values for word similarity is to ask humans to judge how similar one word is to another. A number of datasets have resulted from such experiments. For example the SimLex-999 dataset (Hill et al., 2015) gives values on a scale from 0 to 10, like the examples below, which range from near-synonyms (vanish, disappear) to pairs that scarcely seem to have anything in common (hole, agreement): Consider the meanings of the words coffee and cup. Coffee is not similar to cup; they share practically no features (coffee is a plant or a beverage, while a cup is a manufactured object with a particular shape). But coffee and cup are clearly related; they are associated by co-participating in an everyday event (the event of drinking coffee out of a cup). Similarly scalpel and surgeon are not similar but are related eventively (a surgeon tends to make use of a scalpel).","he notion of word similarity is very useful in larger semantic tasks. Knowing similarity how similar two words are can help in computing how similar the meaning of two phrases or sentences are, a very important component of tasks like question answering, paraphrasing, and summarization."
Vector Semantics and Embeddings,Word2vec,Other kinds of static embeddings,"What is a problem of fasttext, an exension of word2vec?","There are many kinds of static embeddings. An extension of word2vec, fasttext fasttext (Bojanowski et al., 2017) , addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words -words that appear in a test corpus but were unseen in the training corpus. A related problem is word sparsity, such as in languages with rich morphology, where some of the many forms for each noun and verb may only occur rarely. Fasttext deals with these problems by using subword models, representing each word as itself plus a bag of constituent n-grams, with special boundary symbols < and > added to each word. For example, 6.9 • VISUALIZING EMBEDDINGS 119 with n = 3 the word where would be represented by the sequence <where> plus the character n-grams: <wh, whe, her, ere, re>","An extension of word2vec, fasttext fasttext (Bojanowski et al., 2017) , addresses a problem with word2vec as we have presented it so far: it has no good way to deal with unknown words -words that appear in a test corpus but were unseen in the training corpus."
Nonesense,,,Is coffee better or tea?,,*** This is a nonesense question ***
Neural Networks and Neural Language Models,,,Why is the use of neural nets often called deep learning?,"Instead, a modern neural network is a network of small computing units, each of which takes a vector of input values and produces a single output value. In this chapter we introduce the neural net applied to classification. The architecture we introduce is called a feedforward network because the computation proceeds iterfeedforward atively from one layer of units to the next. The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers).","The use of modern neural nets is often called deep learning, because modern networks are often deep (have many layers)."
Neural Networks and Neural Language Models,Units,,What is the building block of a neural network?,"The building block of a neural network is a single computational unit. A unit takes a set of real valued numbers as input, performs some computation on them, and produces an output.",The building block of a neural network is a single computational unit.
Neural Networks and Neural Language Models,The XOR problem,,What is a perceptron?,"This example was first shown for the perceptron, which is a very simple neural perceptron unit that has a binary output and does not have a non-linear activation function. The output y of a perceptron is 0 or 1, and is computed as follows (using the same weight w, input x, and bias b as in Eq. 7.2): ","This example was first shown for the perceptron, which is a very simple neural perceptron unit that has a binary output and does not have a non-linear activation function."
Nonesense,,,Which season comes after Summer?,,*** This is a nonesense question ***
Sequence Labeling for Parts of Speech and Named Entities,,"",What is an abbreviation of Parts of speech?,"Parts of speech (also known as POS) and named entities are useful clues to POS sentence structure and meaning. Knowing whether a word is a noun or a verb tells us about likely neighboring words (nouns in English are preceded by determiners and adjectives, verbs by nouns) and syntactic structure (verbs have dependency links to nouns), making part-of-speech tagging a key aspect of parsing. Knowing if a named entity like Washington is a name of a person, a place, or a university is important to many natural language processing tasks like question answering, stance detection, or information extraction. In this chapter we'll introduce the task of part-of-speech tagging, taking a sequence of words and assigning each word a part of speech like NOUN or VERB, and the task of named entity recognition (NER), assigning words or phrases tags like PERSON, LOCATION, or ORGANIZATION.",Parts of speech (also known as POS)
Sequence Labeling for Parts of Speech and Named Entities,Further Details,Rule-based Methods,What is the difference between academic and commercial approaches to NER?,"While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013) . For example in the IBM System T architecture, a user specifies declarative constraints for tagging tasks in a formal query language that includes regular expressions, dictionaries, semantic constraints, and other operators, which the system compiles into an efficient extractor (Chiticariu et al., 2018) .","While machine learned (neural or CRF) sequence models are the norm in academic research, commercial approaches to NER are often based on pragmatic combinations of lists and rules, with some smaller amount of supervised machine learning (Chiticariu et al., 2013) ."
Nonesense,,,Who wrote Harry Potter?,,*** This is a nonesense question ***
Deep Learning Architectures for Sequence Processing,Recurrent Neural Networks,,What is another name for a simple recurrent network?,"A recurrent neural network (RNN) is any network that contains a cycle within its network connections, meaning that the value of some unit is directly, or indirectly, dependent on its own earlier outputs as an input. While powerful, such networks are difficult to reason about and to train. However, within the general class of recurrent networks there are constrained architectures that have proven to be extremely effective when applied to language. In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman Networks works. These networks are useful in their own right and serve as the basis for more complex approaches like the Long Short-Term Memory (LSTM) networks discussed later in this chapter. In this chapter when we use the term RNN we'll be referring to these simpler more constrained networks (although you will often see the term RNN to mean any net with recurrent properties including LSTMs).","In this section, we consider a class of recurrent networks referred to as Elman Networks (Elman, 1990) or simple recurrent net-Elman Networks works."
Deep Learning Architectures for Sequence Processing,Recurrent Neural Networks,Training,What can we do when the input sequences are too long?,"For applications that involve much longer input sequences, such as speech recognition, character-level processing, or streaming of continuous inputs, unrolling an entire input sequence may not be feasible. In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item."," In these cases, we can unroll the input into manageable fixed-length segments and treat each segment as a distinct training item."
Transfer Learning with Pretrained Language Models and Contextual Embeddings,,,What is the process of pretraining?,"Second, we'll introduce in this chapter the idea of pretraining and fine-tuning. We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text. We'll call these pretrained models pretrained language models, since they can take the form of the transformer language models we introduced in Chapter 9. We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some downstream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task. The pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. Of course, adding grounding from vision or from real-world interaction into pretrained models can help build even more powerful models, but even text alone is remarkably useful, and we will limit our attention here to purely textual models. There are two common paradigms for pretrained language models. One is the causal or left-to-right transformer model we introduced in Chapter 9. In this chapter we'll introduce a second paradigm, called the bidirectional transformer encoder, and the method of masked language modeling, introduced with the BERT model (Devlin et al., 2019 ) that allows the model to see entire texts at a time, including both the right and left context.", We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text.
Transfer Learning with Pretrained Language Models and Contextual Embeddings,,,What is transfer learning?,"Second, we'll introduce in this chapter the idea of pretraining and fine-tuning. We call pretraining the process of learning some sort of representation of meaning for words or sentences by processing very large amounts of text. We'll call these pretrained models pretrained language models, since they can take the form of the transformer language models we introduced in Chapter 9. We call fine-tuning the process of taking the representations from these pretrained models, and further training the model, often via an added neural net classifier, to perform some downstream task like named entity tagging or question answering or coreference. The intuition is that the pretraining phase learns a language model that instantiates a rich representations of word meaning, that thus enables the model to more easily learn ('be fine-tuned to') the requirements of a downstream language understanding task. The pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. Of course, adding grounding from vision or from real-world interaction into pretrained models can help build even more powerful models, but even text alone is remarkably useful, and we will limit our attention here to purely textual models. There are two common paradigms for pretrained language models. One is the causal or left-to-right transformer model we introduced in Chapter 9. In this chapter we'll introduce a second paradigm, called the bidirectional transformer encoder, and the method of masked language modeling, introduced with the BERT model (Devlin et al., 2019 ) that allows the model to see entire texts at a time, including both the right and left context.","The pretrain-finetune paradigm is an instance of what is called transfer learning in machine learning: the method of acquiring knowledge from one task or domain, and then applying it (transferring it) to solve a new task. "
Transfer Learning with Pretrained Language Models and Contextual Embeddings,Bidirectional Transformer Encoders,,How is contextualization applied for models such as BERT?,"This contextualization is accomplished through the use of the same self-attention mechanism used in causal models. As with these models, the first step is to generate a set of key, query and value embeddings for each element of the input vector x through the use of learned weight matrices W Q , W K , and W V . These weights project each input vector x i into its specific role as a key, query, or value.",This contextualization is accomplished through the use of the same self-attention mechanism used in causal models.
Nonesense,,,Is Leiden located in the Netherlands?,,*** This is a nonesense question ***
Machine Translation and Encoder-Decoder Models,,,What is MT?,"This chapter introduces machine translation (MT), the use of computers to translate from one language to another. Of course translation, in its full generality, such as the translation of literature, or poetry, is a difficult, fascinating, and intensely human endeavor, as rich as any other area of human creativity.",the use of computers to translate from one language to another.
Machine Translation and Encoder-Decoder Models,,,What is the task of CAT?,Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing post-editing phase by a human translator. This task is often called computer-aided translation or CAT. CAT is commonly used as part of localization: the task of adapting content CAT localization or a product to a particular language community.,Another common use of machine translation is to aid human translators. MT systems are routinely used to produce a draft translation that is fixed up in a post-editing post-editing phase by a human translator.
Machine Translation and Encoder-Decoder Models,The Encoder-Decoder Model,,What is another term for Encoder-decoder networks?,"Encoder-decoder networks, or sequence-to-sequence networks, are models ca-encoderdecoder pable of generating contextually appropriate, arbitrary length, output sequences. Encoder-decoder networks have been applied to a very wide range of applications including machine translation, summarization, question answering, and dialogue.","Encoder-decoder networks, or sequence-to-sequence networks,"
Nonesense,,,When did Prince Harry marry Meghan Markle?,,*** This is a nonesense question ***