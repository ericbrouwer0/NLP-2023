zero or more occurrences of the immediately previous character or regular expression
pattern
tokenizing (segmenting) words
token learner, and token segmenter
token segmenter is used to segment a test sentence into tokens in a vocabulary.
learning a vocabulary
the task of putting words/tokens in a standard format
lemmatization is performed by complete morphological parsing of the word.
the task of determining that two words have the same root, despite their surface differences
the minimum number of operations it takes to edit one into the other.
a neural network language model
by computing the probability of a complete word sequence.
a model Markov chain
counting in a corpus and normalizing
bigram models are more accurate, but trigram models are more accurate.
how well they fit the test set
the inverse probability of the test set, normalized by the number of words
by using the chain rule
pruning
if the probabilities assigned to a Stationary sequence are invariant with respect to
because it is too hard to compute directly without simplifying assumptions
.
features from the input are most useful
because accuracy doesn't work well when the classes are unbalanced
k disjoint subsets
supervised machine learning classifier
an online algorithm that minimizes the loss function
an online algorithm that minimizes the loss function
gradient descent
softmax regression
affective meanings
meaning is related to the distribution of words in context
each word type in the vocabulary
documents
because it will tend to be high just when the two vectors have large values in the same
the best way to weigh the association between two words is to ask how much more the two words
a single vector
input units, hidden units, and output units
a probability distribution over words
a representation of the process of computing a mathematical expression
assigning a part-of-speech label to each of a sequence of
anything that can be referred to with a proper name
a hidden Markov model allows us to talk about both observed events hidden Markov model (
many language tasks require access to information that can be arbitrarily distant from the current word.
any network that contains a cycle within its network connections
a model that predicts a value at time t based on a linear
removing information no longer needed from the context, and adding information likely to be needed for later
to delete information from the context that is no longer needed
relevance in the current context
to avoid numerical issues and effective loss of gradients during training
self-attention layer, feedforward layer, norm, and layer normalization
no word or phrase, short of an explanatory footnote, can express the exact meaning
languages that can omit pronouns
beam search
head 1  head 2 ...  head h
chrF stands for character F-score
80%
to predict the original inputs for each of the masked tokens using a bi
sentence embedding
